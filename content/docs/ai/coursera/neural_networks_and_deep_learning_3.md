---
title: coursera - Neural Networks and Deep Learning - (3) multiple layer
type: blog
prev: /
next: docs/folder/
math: true
---

## Intro

ì‹ ê²½ë§ì€ ë‹¨ìˆœí•œ í¼ì…‰íŠ¸ë¡  ëª¨ë¸ì—ì„œ ì‹œì‘í•˜ì—¬ ì ì  ê¹Šì–´ì§ˆìˆ˜ë¡ ë” ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ëœë‹¤.
ì´ì „ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë‘ ê°œì˜ ì¸µì„ ê°€ì§„ ì‹ ê²½ë§(2-Layer neural network)ë¥¼ êµ¬í˜„í–ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì‹ ê²½ë§ì€ ë” ê¹Šì€ ì¸µì„ ê°€ì§€ê³  ìˆë‹¤.

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Lê°œì˜ Hidden layerë¥¼ ê°€ì§„ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ë©´ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ í™•ì¸í•˜ê³ ì í•œë‹¤.

- ì—¬ëŸ¬ê°œì˜ ì¸µì„ ê°–ëŠ” Deep Neural Network(DNN)ì˜ êµ¬ì¡°
- Forward propagation & Backward propagation ê°œë… í™•ì¥
- ë‹¤ì¸µ ì‹ ê²½ë§ì—ì„œ Weight, Bias ì´ˆê¸°í™” ë° ì—…ë°ì´íŠ¸
- Relu, sigmoid ë“± ë‹¤ì–‘í•œ í™œì„±í™” í•¨ìˆ˜ ì ìš©

## 1. íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”

L-layerë¥¼ ê°€ì§„ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ë ¤ë©´, ê° ì¸µì˜ Weight, biasë¥¼ ì´ˆê¸°í™”í•´ì•¼ í•œë‹¤.
ë‹¤ìŒì˜ í•¨ìˆ˜ëŠ” layer_dimsë¥¼ ë°›ì•„ì„œ ê° ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ˆê¸°í™” í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

```python
def initialize_parameters(layer_dims):
    np.random.seed(1)
    parameters = {}

    L = len(layer_dims)

    for l in range(1,L):
        parameters['W'+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01
        parameters['b'+str(l)] = np.zeros((layer_dims[l],1))

    return parameters
```

## 2. Forward Propagation

ì‹ ê²½ë§ì€ ì—¬ëŸ¬ê°œì˜ ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆê³ , ì…ë ¥ -> Linear -> Activation ì˜ ê³¼ì •ì„ ê±°ì¹œë‹¤.

```python
def linear_forward(A, W, b):
    Z = np.dot(A, W) + b
    cache = (A, W, b)

    return Z, cache

def linear_activation_forward(A_prev, W, b, activation):
    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev, W, b) # linear_cache : (A, W, b)
        A, activation_cache = sigmoid(Z) # activation_cache : Z
    if activation == "relu":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(z)

def L_Model_Forward(X, parameters):
    caches = []
    A = X
    L = len(parameters) // 2

    for l in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu') # cache : ((A, W, b), Z)
        caches.append(cache)

    AL, cache = inear_activation_forward(A_prev, parameters['W' + str(L)], parameters['b' + str(L)], 'sigmoid')
    caches.append(cache)

    return AL, caches

```

### cacheë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ 

![figure1](./assets/why-forward-cached.png)

```python
# ğŸ”´ ì˜ëª»ëœ ì ‘ê·¼ (ë‹¤ì‹œ ê³„ì‚°í•˜ë©´ ë¹„íš¨ìœ¨ì )
def backward_propagation(dA, W, A_prev, Z):
    dZ = dA * activation_derivative(Z)  # ë‹¤ì‹œ ê³„ì‚° í•„ìš”
    dW = np.dot(dZ, A_prev.T) / m  # ë‹¤ì‹œ ê³„ì‚° í•„ìš”
    db = np.sum(dZ, axis=1, keepdims=True) / m  # ë‹¤ì‹œ ê³„ì‚° í•„ìš”
    return dW, db

# ğŸŸ¢ ì˜¬ë°”ë¥¸ ì ‘ê·¼ (cacheë¥¼ í™œìš©í•˜ë©´ íš¨ìœ¨ì )
def backward_propagation(dA, cache):
    A_prev, W, b, Z = cache  # Forwardì—ì„œ ì €ì¥ëœ ê°’ ë¶ˆëŸ¬ì˜¤ê¸°
    dZ = dA * activation_derivative(Z)
    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m
    return dW, db

```

## Cost ê³„ì‚°í•˜ê¸°

ë”¥ëŸ¬ë‹ì—ì„œ ì´ì§„ ë¶„ë¥˜(Binary Classification) ë¥¼ ìˆ˜í–‰í•  ë•Œ, ëª¨ë¸ì˜ ì¶œë ¥ì¸µì—ì„œ ì‹œê·¸ëª¨ì´ë“œ(Sigmoid) í•¨ìˆ˜ê°€ ì‚¬ìš©ëœë‹¤.
ì´ ê²½ìš°, í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ë¹„ìš© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.
ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ê°’ AL ì´ ì‹¤ì œ ê°’ Y ì™€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ì§€ë¥¼ ì¸¡ì •í•œë‹¤.

```python
def compute_cost(AL, Y):
    m = Y.shape[1]  # Number of examples

    cost = - (1/m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))
    cost = np.squeeze(cost)

    return cost

```

## 4. Backward Propagation

ì‹ ê²½ë§ì´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ì†ì‹¤ì„ ì¤„ì´ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜(W)ì™€ í¸í–¥(b)ë¥¼ ì¡°ì •í•´ì•¼ í•œë‹¤. ì—­ì „íŒŒ(Backward Propagation)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸°(Gradient)ë¥¼ êµ¬í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

Backward Propagation ê³µì‹ ìœ ë„ëŠ” [ì´ì „ê¸€](https://soonyubi.github.io/docs/ai/coursera/neural_networks_and_deep_learning_2/#back-propagation)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

```python
def sigmoid_backward(dA, activation_cache):
    Z = activation_cache
    s = 1 / (1 + np.exp(-Z))
    dZ = dA * s * (1-s)
    return dZ

def relu_backward(dA, activation_cache):
    Z = activation_cache
    dZ = np.array(dA, copy=True)
    dZ[Z<0] = 0
    return dZ



def linear_activation_backward(dA, cache, activation):
    linear_cache, activation_cache = cache

    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    return dA_prev, dW, db

def L_model_backward(AL, Y, caches):

    grads = {}
    L = len(caches)  # Number of layers
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)  # Ensure Y has the same shape as AL


    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))


    current_cache = caches[L-1]
    dA_prev, dW, db = linear_activation_backward(dAL, current_cache, "sigmoid")

    grads["dA" + str(L-1)] = dA_prev
    grads["dW" + str(L)] = dW
    grads["db" + str(L)] = db


    for l in reversed(range(L-1)):
        current_cache = caches[l]
        dA_prev, dW, db = linear_activation_backward(grads["dA" + str(l+1)], current_cache, "relu")

        grads["dA" + str(l+1)] = dA_prev
        grads["dW" + str(l+1)] = dW
        grads["db" + str(l+1)] = db

    return grads

```

## 5. update parameters

ì´ í•¨ìˆ˜ëŠ” ì—­ì „íŒŒ(Backward Propagation)ì—ì„œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°(Gradient)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜(W)ì™€ í¸í–¥(b)ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì—­í• ì„ í•œë‹¤.

ì‹ ê²½ë§ì´ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ì†ì‹¤(Cost)ì„ ì¤„ì´ë ¤ë©´, ê°€ì¤‘ì¹˜(W)ì™€ í¸í–¥(b)ë¥¼ ì¡°ì •í•´ì•¼ í•œë‹¤.
ì´ë¥¼ ìœ„í•´ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•˜ì—¬ ê¸°ìš¸ê¸°(Gradient) ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒì´ update_parameters() í•¨ìˆ˜ì˜ ì—­í• ì´ë‹¤.

```python
def update_parameters(parameters, grads, learning_rate):
    L = len(parameters) // 2  # Number of layers in the network

    for l in range(1, L + 1):
        parameters["W" + str(l)] -= learning_rate * grads["dW" + str(l)]
        parameters["b" + str(l)] -= learning_rate * grads["db" + str(l)]

    return parameters
```
