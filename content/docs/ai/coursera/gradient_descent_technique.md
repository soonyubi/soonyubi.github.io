---
title: ê²½ì‚¬ í•˜ê°•ë²•ì˜ ëª¨ë“  ê²ƒ GD, SGD, Mini-batch GD, ê·¸ë¦¬ê³  Adamê¹Œì§€
type: blog
prev: docs/folder
next: docs/folder
math: true
---

## Introduction

ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ëª©í‘œëŠ” **ë¹„ìš©í•¨ìˆ˜** ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.
ì´ê±¸ ìœ„í•´ ìš°ë¦¬ëŠ” ë§¤ iteration ë§ˆë‹¤ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê¸ˆì”© ìˆ˜ì •í•˜ëŠ”ë°, ê·¸ ê³¼ì •ì„ ë‹´ë‹¹í•˜ëŠ” ê²ƒì´ ê²½ì‚¬í•˜ê°•ë²•ì´ë‹¤.

í•˜ì§€ë§Œ ê²½ì‚¬í•˜ê°•ë²•ì€ í•˜ë‚˜ì˜ ë°©ì‹ë§Œ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤.
ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•˜ëƒì— ë”°ë¼ ë‹¤ì–‘í•œ ë°©ë²•ì´ ì¡´ì¬í•œë‹¤.

ì´ë²ˆ ê¸€ì—ì„œëŠ” ê²½ì‚¬í•˜ê°•ë²•ì˜ ì„¸ê°€ì§€ ëŒ€í‘œì ì¸ ë°©ì‹ì¸ **Batch Gradient Descent (GD), Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent** ì´ ì„¸ ê°€ì§€ë¥¼ ë¹„êµí•´ë³´ê³ , ê°ê°ì˜ íŠ¹ì§•ê³¼ ì¥ë‹¨ì ì„ ì°¨ê·¼ì°¨ê·¼ ì•Œì•„ë³¼ ê±°ë‹¤.

## GD vs SGD vs Mini-batch GD: ê°œë…

![figure2](./assets/gd_sgd_minibatch.png)

### Batch Gradient Descent

- **ì •ì˜**: ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ëª¨ë‘ ì‚¬ìš©í•´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
- **ì—…ë°ì´íŠ¸ íšŸìˆ˜**ëŠ” ì ì§€ë§Œ, í•œ ë²ˆì˜ ê³„ì‚°ì— **ë¹„ìš©ì´ í½ë‹ˆë‹¤**.
- **êµ¬í˜„ ë°©ì‹ ìš”ì•½**:
  ```python
  for i in range(num_iterations):
      AL, caches = forward_propagation(X, parameters)
      grads = backward_propagation(AL, Y, caches)
      parameters = update_parameters(parameters, grads)
  ```

### Stochastic Gradient Descent

- **ì •ì˜**: í•œ ë²ˆì— ë”± í•˜ë‚˜ì˜ í›ˆë ¨ ì˜ˆì œë§Œ ì‚¬ìš©í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
- êµ‰ì¥íˆ ìì£¼ ì—…ë°ì´íŠ¸í•˜ì§€ë§Œ, **ë°©í–¥ì´ ìì£¼ í”ë“¤ë¦½ë‹ˆë‹¤.**
- **êµ¬í˜„ ë°©ì‹ ìš”ì•½**:
  ```python
  for i in range(num_iterations):
      for j in range(m):  # m = number of training examples
          x_j = X[:, j].reshape(-1, 1)
          y_j = Y[:, j].reshape(1, 1)
          AL, caches = forward_propagation(x_j, parameters)
          grads = backward_propagation(AL, y_j, caches)
          parameters = update_parameters(parameters, grads)
  ```

### Mini-Batch Gradient Descent

- **ì •ì˜**: ë°ì´í„°ë¥¼ ì¼ì • í¬ê¸°ì˜ ë¬¶ìŒ(mini-batch)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµí•©ë‹ˆë‹¤.  
  ì˜ˆ: í•œ ë²ˆì— 32ê°œ, 64ê°œì”© ì‚¬ìš©
- **ì†ë„ì™€ ì•ˆì •ì„±ì˜ ê· í˜•**ì´ ê°€ì¥ ì˜ ì¡íŒ ë°©ì‹ì…ë‹ˆë‹¤.
- **êµ¬í˜„ ë°©ì‹ ìš”ì•½**:

  ```python
  def random_mini_batches(X, Y, mini_batch_size=64, seed=0):
    np.random.seed(seed)
    m = X.shape[1]  # ì´ ìƒ˜í”Œ ìˆ˜
    mini_batches = []

    # Step 1: ë°ì´í„° ì„ê¸°
    permutation = list(np.random.permutation(m))
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape(Y.shape[0], m)

    # Step 2: ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    num_complete_minibatches = m // mini_batch_size
    for k in range(num_complete_minibatches):
        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        mini_batches.append((mini_batch_X, mini_batch_Y))

    # ë‚¨ì€ ìƒ˜í”Œ ì²˜ë¦¬ (ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ ë–¨ì–´ì§€ì§€ ì•Šì„ ê²½ìš°)
    if m % mini_batch_size != 0:
        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size :]
        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size :]
        mini_batches.append((mini_batch_X, mini_batch_Y))

    return mini_batches

  mini_batches = random_mini_batches(X, Y, mini_batch_size=64)

  for i in range(num_iterations):
      for mini_batch in mini_batches:
          (mini_batch_X, mini_batch_Y) = mini_batch
          AL, caches = forward_propagation(mini_batch_X, parameters)
          grads = backward_propagation(AL, mini_batch_Y, caches)
          parameters = update_parameters(parameters, grads)
  ```

#### â“ ì™œ mini-batch í¬ê¸°ë¥¼ 2ì˜ ê±°ë“­ì œê³±(ì˜ˆ: 32, 64, 128 ë“±)ìœ¼ë¡œ ìë¥¼ê¹Œ?

ì´ëŠ” í•˜ë“œì›¨ì–´, íŠ¹íˆ GPU/TPUì—ì„œì˜ ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ë•Œë¬¸ì…ë‹ˆë‹¤.
2ì˜ ê±°ë“­ì œê³± í¬ê¸°ëŠ” ë©”ëª¨ë¦¬ ì •ë ¬ ë° ì—°ì‚° ë‹¨ìœ„ì™€ ì˜ ë§ê¸° ë•Œë¬¸ì—,
â€¢ ê³„ì‚°ì´ ë” ë¹ ë¥´ê³ 
â€¢ ë©”ëª¨ë¦¬ ì ‘ê·¼ ì†ë„ë„ í–¥ìƒë¨

ë”°ë¼ì„œ ë§ì€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬(PyTorch, TensorFlow ë“±)ì—ì„œë„ ê¸°ë³¸ì ìœ¼ë¡œ batch_size=32 í˜¹ì€ 64ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.

#### âš–ï¸ mini-batch í¬ê¸°ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„

â— ë„ˆë¬´ ì‘ìœ¼ë©´?
â€¢ ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ê°€ ì ì–´ì„œ gradient(ê¸°ìš¸ê¸°)ì˜ ë°©í–¥ì´ í”ë“¤ë¦¼
â€¢ ì´ê±¸ ì§„ë™(oscillation) ì´ë¼ê³  ë¶€ë¥´ëŠ”ë°,
ëª¨ë¸ì´ ì¼ì •í•œ ë°©í–¥ìœ¼ë¡œ ì˜ ìˆ˜ë ´í•˜ì§€ ëª»í•˜ê³  ì™”ë‹¤ ê°”ë‹¤ í•  ìˆ˜ ìˆìŒ

â— ë„ˆë¬´ í¬ë©´?
â€¢ GDì™€ ë‹¤ë¥¼ ë°” ì—†ì´ ì „ì²´ ë°ì´í„°ì— ê°€ê¹Œì›Œì§
â€¢ ì—…ë°ì´íŠ¸ê°€ ë„ˆë¬´ ëŠë ¤ì§€ê³ , ì†ë„ ì´ì ì´ ì¤„ì–´ë“¦
â€¢ ìì£¼ ì—…ë°ì´íŠ¸í•˜ì§€ ëª»í•˜ë‹ˆ ë¹ ë¥´ê²Œ ìµœì ì ì„ ì°¾ê¸° ì–´ë ¤ì›€

## Momentum

Mini-batch Gradient DescentëŠ” ì „ì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì¼ë¶€ ë°ì´í„°ë§Œìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ë•Œë¬¸ì—,  
Gradient(ê¸°ìš¸ê¸°)ì˜ ë°©í–¥ì´ **ì¼ì •í•˜ì§€ ì•Šê³  ì§„ë™**í•˜ëŠ” í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ëŸ¬í•œ ì§„ë™ì€ ëª¨ë¸ì´ **ìµœì†Ÿê°’ì— ë¹ ë¥´ê²Œ ë„ë‹¬í•˜ì§€ ëª»í•˜ê³ **,  
ì˜¤íˆë ¤ ìµœì†Ÿê°’ ì£¼ë³€ì„ ì™”ë‹¤ ê°”ë‹¤ í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ëŒ€í‘œì ì¸ ê¸°ë²•ì´ ë°”ë¡œ **Momentum**ì…ë‹ˆë‹¤.

Momentum(ëª¨ë©˜í…€)ì€ **ì´ì „ ë‹¨ê³„ì˜ Gradient ë°©í–¥ì„ ê¸°ì–µí•˜ì—¬**,  
í˜„ì¬ì˜ Gradientì™€ **ê²°í•©ëœ ë°©í–¥**ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

ë§ˆì¹˜ **ê²½ì‚¬ë©´ì„ ë‚´ë ¤ê°€ëŠ” ê³µ**ì´ ì´ì „ì— ì–»ì€ **ì†ë„(ë°©í–¥ + í¬ê¸°)** ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©° ì ì  ê°€ì†ë„ ë¶™ì—¬ì„œ ë‚´ë ¤ì˜¤ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤.

\[
v = \beta \cdot v + (1 - \beta) \cdot \nabla J
\]
\[
\theta = \theta - \alpha \cdot v
\]

- \( v \): ì´ì „ ë‹¨ê³„ê¹Œì§€ì˜ ëˆ„ì ëœ Gradient (ì†ë„)
- \( \beta \): ëª¨ë©˜í…€ ê³„ìˆ˜ (ë³´í†µ 0.9)
- \( \nabla J \): í˜„ì¬ì˜ Gradient
- \( \alpha \): í•™ìŠµë¥  (learning rate)

```python
def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    Momentumì„ ì ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

    Arguments:
    parameters -- ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° (W, b ë“±)
    grads -- ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸
    v -- ì´ì „ ë‹¨ê³„ê¹Œì§€ì˜ ì†ë„(ëˆ„ì ëœ ê·¸ë˜ë””ì–¸íŠ¸ í‰ê· )
    beta -- ëª¨ë©˜í…€ ê³„ìˆ˜ (ë³´í†µ 0.9)
    learning_rate -- í•™ìŠµë¥ 

    Returns:
    parameters -- ì—…ë°ì´íŠ¸ëœ íŒŒë¼ë¯¸í„°
    v -- ì—…ë°ì´íŠ¸ëœ ì†ë„ ë²¡í„°
    """
    L = len(parameters) // 2  # ë ˆì´ì–´ ìˆ˜

    for l in range(1, L + 1):
        # 1. ì†ë„ ì—…ë°ì´íŠ¸: ì´ì „ ì†ë„ì™€ í˜„ì¬ ê·¸ë˜ë””ì–¸íŠ¸ì˜ ê²°í•©
        v["dW" + str(l)] = beta * v["dW" + str(l)] + (1 - beta) * grads["dW" + str(l)]
        v["db" + str(l)] = beta * v["db" + str(l)] + (1 - beta) * grads["db" + str(l)]

        # 2. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: ì†ë„ë¥¼ ë°˜ì˜í•˜ì—¬ íŒŒë¼ë¯¸í„° ì´ë™
        parameters["W" + str(l)] -= learning_rate * v["dW" + str(l)]
        parameters["b" + str(l)] -= learning_rate * v["db" + str(l)]

    return parameters, v
```

#### ëª¨ë©˜í…€ ê³„ìˆ˜ Î²ì˜ ì—­í• 

- Î² = 0ì´ë©´ ì¼ë°˜ì ì¸ Gradient Descentì™€ ë™ì¼í•´ì§‘ë‹ˆë‹¤.
- Î²ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê³¼ê±° ë°©í–¥ì„ ë” ë§ì´ ë°˜ì˜í•˜ì—¬ ì§„ë™ì´ ì¤„ì–´ë“¤ê³  ë¶€ë“œëŸ¬ìš´ ìˆ˜ë ´ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ì¼ë°˜ì ìœ¼ë¡œëŠ” Î² = 0.9ë¥¼ ê°€ì¥ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.

## Adam Optimizer: Momentumê³¼ RMSPropì˜ ë§Œë‚¨

ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ **Adam(Adaptive Moment Estimation)** ì…ë‹ˆë‹¤.  
Adamì€ ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ë‘ ê°€ì§€ ê¸°ë²•ì„ ë™ì‹œì— ì‚¬ìš©í•©ë‹ˆë‹¤:

1. **Momentum** â†’ ê³¼ê±°ì˜ ê¸°ìš¸ê¸° ë°©í–¥ì„ ë°˜ì˜
2. **RMSProp** â†’ ê¸°ìš¸ê¸°ì˜ í¬ê¸°ì— ë”°ë¼ í•™ìŠµë¥ ì„ ìë™ ì¡°ì ˆ

ì¦‰,

> **ë°©í–¥ì€ ëª¨ë©˜í…€ì²˜ëŸ¼ ì¡ê³ , ì†ë„ëŠ” RMSPropì²˜ëŸ¼ ì¡°ì ˆí•œë‹¤**

### ìˆ˜ì‹

Adamì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ì„ ë”°ë¦…ë‹ˆë‹¤ (ë ˆì´ì–´ \( l \)):

\[
\begin{aligned}
v*{dW^{[l]}} &= \beta_1 \cdot v*{dW^{[l]}} + (1 - \beta*1) \cdot \frac{\partial J}{\partial W^{[l]}} \\
v^{\text{corrected}}*{dW^{[l]}} &= \frac{v*{dW^{[l]}}}{1 - \beta_1^t} \\[10pt]
s*{dW^{[l]}} &= \beta*2 \cdot s*{dW^{[l]}} + (1 - \beta*2) \cdot \left( \frac{\partial J}{\partial W^{[l]}} \right)^2 \\
s^{\text{corrected}}*{dW^{[l]}} &= \frac{s*{dW^{[l]}}}{1 - \beta_2^t} \\[10pt]
W^{[l]} &= W^{[l]} - \alpha \cdot \frac{v^{\text{corrected}}*{dW^{[l]}}}{\sqrt{s^{\text{corrected}}\_{dW^{[l]}}} + \varepsilon}
\end{aligned}
\]

- \( v \): 1ì°¨ ëª¨ë©˜íŠ¸(=ê¸°ìš¸ê¸°ì˜ í‰ê· )
- \( s \): 2ì°¨ ëª¨ë©˜íŠ¸(=ê¸°ìš¸ê¸°ì˜ ì œê³± í‰ê· )
- \( \beta_1, \beta_2 \): ê°ê° 1ì°¨, 2ì°¨ ëª¨ë©˜íŠ¸ì˜ ì§€ìˆ˜í‰ê·  ê³„ìˆ˜ (ë³´í†µ 0.9, 0.999)
- \( t \): í˜„ì¬ ë°˜ë³µ íšŸìˆ˜
- \( \varepsilon \): 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ì‘ì€ ìˆ˜ (ë³´í†µ \( 10^{-8} \))

### ğŸ§  ì™œ Adamì´ ê°•ë ¥í•œê°€ìš”?

| ìš”ì†Œ               | ì„¤ëª…                                                                        |
| ------------------ | --------------------------------------------------------------------------- |
| **ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„** | ëª¨ë©˜í…€ ë•ë¶„ì— ë¹ ë¥´ê²Œ ì „ì§„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.                                    |
| **ì§„ë™ ì–µì œ**      | RMSPropì˜ ì œê³± í‰ê·  ë•ë¶„ì— ë°©í–¥ì´ í”ë“¤ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.                        |
| **íŠœë‹ì´ ì‰¬ì›€**    | ê¸°ë³¸ê°’ë§Œìœ¼ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.                            |
| **ì‹¤ë¬´ ì ìš©ì„±**    | ë‹¤ì–‘í•œ ë¬¸ì œì— ë‘ë£¨ ì“°ì´ë©°, ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì˜ ê¸°ë³¸ Optimizerë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. |

### Adam ì½”ë“œ êµ¬í˜„ ì˜ˆì‹œ

ì•„ë˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ Adam ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ì½”ë“œ ì˜ˆì‹œì…ë‹ˆë‹¤:

```python
def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,
                                beta1=0.9, beta2=0.999, epsilon=1e-8):
    L = len(parameters) // 2
    v_corrected = {}
    s_corrected = {}

    for l in range(1, L + 1):
        # 1. Moving average of the gradients
        v["dW" + str(l)] = beta1 * v["dW" + str(l)] + (1 - beta1) * grads["dW" + str(l)]
        v["db" + str(l)] = beta1 * v["db" + str(l)] + (1 - beta1) * grads["db" + str(l)]

        # 2. Bias correction
        v_corrected["dW" + str(l)] = v["dW" + str(l)] / (1 - beta1 ** t)
        v_corrected["db" + str(l)] = v["db" + str(l)] / (1 - beta1 ** t)

        # 3. Moving average of the squared gradients
        s["dW" + str(l)] = beta2 * s["dW" + str(l)] + (1 - beta2) * (grads["dW" + str(l)] ** 2)
        s["db" + str(l)] = beta2 * s["db" + str(l)] + (1 - beta2) * (grads["db" + str(l)] ** 2)

        # 4. Bias correction
        s_corrected["dW" + str(l)] = s["dW" + str(l)] / (1 - beta2 ** t)
        s_corrected["db" + str(l)] = s["db" + str(l)] / (1 - beta2 ** t)

        # 5. Parameter update
        parameters["W" + str(l)] -= learning_rate * (
            v_corrected["dW" + str(l)] / (np.sqrt(s_corrected["dW" + str(l)]) + epsilon)
        )
        parameters["b" + str(l)] -= learning_rate * (
            v_corrected["db" + str(l)] / (np.sqrt(s_corrected["db" + str(l)]) + epsilon)
        )

    return parameters, v, s
```

### â“ Bias Correctionì´ ì™œ í•„ìš”í• ê¹Œ?

Adamì€ **ê¸°ìš¸ê¸°ì˜ ì§€ìˆ˜ ê°€ì¤‘ í‰ê· **ì„ ì‚¬ìš©í•˜ëŠ”ë°,  
ì´ ì§€ìˆ˜ í‰ê· ì€ **ì´ˆê¸°ê°’ì´ 0**ìœ¼ë¡œ ì‹œì‘í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµ ì´ˆê¸°ì— **ê°’ì´ ì‘ê²Œ ë‚˜ì˜¤ëŠ” í¸í–¥(bias)** ì´ ìƒê¹ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë©˜í…€ì˜ ëˆ„ì  í‰ê·  \( v \)ë¥¼ ë³´ë©´:

\[
v = \beta_1 \cdot v + (1 - \beta_1) \cdot g
\quad \text{(ì—¬ê¸°ì„œ } g \text{ëŠ” í˜„ì¬ ê·¸ë˜ë””ì–¸íŠ¸)}
\]

ì²˜ìŒ ëª‡ ë²ˆì€ \( v \)ê°€ 0ì— ê°€ê¹Œìš´ ìƒíƒœì—ì„œ ì¶œë°œí•˜ê¸° ë•Œë¬¸ì—  
ì‹¤ì œ ê·¸ë˜ë””ì–¸íŠ¸ì˜ í‰ê· ë³´ë‹¤ **ê³¼ì†Œ ì¶”ì •**í•˜ê²Œ ë©ë‹ˆë‹¤.

---

### ğŸ” ì˜ˆì‹œë¡œ ì´í•´í•´ë³´ê¸°

- \( \beta_1 = 0.9 \), \( t = 1 \)ì¼ ë•Œ:
  \[
  v = 0.9 \cdot 0 + 0.1 \cdot g = 0.1g
  \]
- ë‹¨ 1ë²ˆë§Œ ì—…ë°ì´íŠ¸í–ˆëŠ”ë°, ì‹¤ì œ ê¸°ìš¸ê¸°ë³´ë‹¤ **10ë°° ì‘ê²Œ ë°˜ì˜**ë˜ëŠ” ê²ƒì´ì£ .

---

### âœ… í•´ê²° ë°©ë²•: Bias Correction

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Adamì€ **bias correction(í¸í–¥ ë³´ì •)** ì„ ì ìš©í•©ë‹ˆë‹¤.

\[
v^{\text{corrected}} = \frac{v}{1 - \beta_1^t}
\]

ì´ ë³´ì •ì‹ì„ í†µí•´ ì´ˆê¸°ì— 0ì—ì„œ ì‹œì‘í•˜ë”ë¼ë„  
**ì‹¤ì œ ê¸°ëŒ€ê°’ì— ê°€ê¹Œìš´ í‰ê· ê°’**ìœ¼ë¡œ ë³´ì •í•´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê°™ì€ ë°©ì‹ìœ¼ë¡œ, ì œê³± í‰ê· ì„ ì €ì¥í•˜ëŠ” \( s \)ì—ë„ ë³´ì •ì„ ì ìš©í•©ë‹ˆë‹¤:

\[
s^{\text{corrected}} = \frac{s}{1 - \beta_2^t}
\]

## Learning Rate Decayì™€ ìŠ¤ì¼€ì¤„ë§

ëª¨ë¸ì´ í•™ìŠµì„ ì˜ í•˜ë ¤ë©´ **í•™ìŠµë¥ (learning rate)** ì„¤ì •ì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.  
ì²˜ìŒì—ëŠ” í¬ê²Œ ì›€ì§ì´ë©° ë¹ ë¥´ê²Œ ìµœì ê°’ì— ì ‘ê·¼í•˜ê³ ,  
ë‚˜ì¤‘ì—ëŠ” ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ìˆ˜ë ´í•´ì•¼ í•˜ì£ .

ì´ëŸ° íë¦„ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ë°”ë¡œ **Learning Rate Decay** ë˜ëŠ” **ìŠ¤ì¼€ì¤„ë§(scheduling)** ì…ë‹ˆë‹¤.

---

### ğŸ”½ Learning Rate Decayë€?

í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ í•™ìŠµë¥ ì„ **ì ì  ì¤„ì—¬ë‚˜ê°€ëŠ” ê¸°ë²•**ì…ë‹ˆë‹¤.

- ì´ˆë°˜ì—ëŠ” ë¹ ë¥´ê²Œ ì›€ì§ì´ë‹¤ê°€
- ì ì  ë” **ì‘ì€ ë³´í­ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •**í•˜ê²Œ ë©ë‹ˆë‹¤.

ì˜ˆì‹œ:  
\[
\alpha_t = \frac{\alpha_0}{1 + decay \cdot t}
\]

- \( \alpha_0 \): ì´ˆê¸° í•™ìŠµë¥ 
- \( t \): í˜„ì¬ epoch ë˜ëŠ” step
- `decay`: ê°ì†Œ ê³„ìˆ˜

---

### ğŸ—“ï¸ Schedulingì´ë€?

í•™ìŠµë¥ ì„ **ì‚¬ì „ì— ì •ì˜í•œ ê·œì¹™ì— ë”°ë¼ ì¡°ì •í•˜ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´:

- **Step Decay**: ì¼ì • epochë§ˆë‹¤ í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê¸°
- **Exponential Decay**: ë§¤ epochë§ˆë‹¤ ì¼ì • ë¹„ìœ¨ë¡œ ê°ì†Œ
- **Reduce on Plateau**: ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œë§Œ í•™ìŠµë¥  ê°ì†Œ

---

### âœ… ì •ë¦¬

| ë°©ì‹                | íŠ¹ì§•                                          |
| ------------------- | --------------------------------------------- |
| Learning Rate Decay | ì „ì²´ í•™ìŠµ íë¦„ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ê°ì†Œ         |
| Scheduling          | ì‚¬ì „ ê·œì¹™ ë˜ëŠ” ì„±ëŠ¥ ë³€í™”ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ì¡°ì • |

ì´ ê¸°ë²•ë“¤ì€ ëŒ€ë¶€ë¶„ì˜ í”„ë ˆì„ì›Œí¬ì—ì„œ ê°„ë‹¨í•˜ê²Œ ì ìš© ê°€ëŠ¥í•˜ë©°,  
**ë” ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ì•ˆì •ì ì¸ ìµœì í™”**ì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
