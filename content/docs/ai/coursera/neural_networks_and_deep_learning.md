---
title: coursera - Neural Networks and Deep Learning - (1) Logistic Regression
type: blog
prev: /
next: docs/folder/
math: true
---

## Intro

logistic regression ì´ neural networkì˜ ê¸°ì´ˆë¼ê³  í•˜ëŠ”ë°
"ì™œ ë‹¨ìˆœí•œ íšŒê·€ ëª¨ë¸ì´ ì‹ ê²½ë§ê³¼ ì—°ê²°ë ê¹Œ?" / "í‘œì¤€í™”ëŠ” ì™œ í•´ì•¼í•˜ê³ , ê°•ì‚¬í•˜ê°•ë²•ì€ ì–´ë–»ê²Œ ë™ìž‘í•˜ëŠ” ê²ƒì¼ê¹Œ?"

ì´ë²ˆ ê¸€ì—ì„œëŠ” ìœ„ ì§ˆë¬¸ì˜ ë‹µì„ ì°¾ê¸° ìœ„í•´ Logistic Regression ì„ êµ¬í˜„í•˜ë©° ì–»ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì •ë¦¬í•˜ê³ ìž í•œë‹¤.

## Logistic Regression with neural network mindset

### preprocessing

1. Figure out the dimensions and shape of problem dataset
2. Reshape dataset
3. Standardize / Normalize dataset

ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì—ì„œ, ì´ë¯¸ì§€ì˜ ê²½ìš° (height, width, channel) ì´ë ‡ê²Œ ë“¤ì–´ì˜¤ëŠ”ë°, ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ (height _ width _ channel, 1) ì˜ shapeë¥¼ ê°€ì§€ë„ë¡ reshape í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.

ì´ìœ ëŠ”, logistic regression ì´ë‚˜ neural networkëŠ” 2ì°¨ì› ë²¡í„°ê°€ ìž…ë ¥ë˜ëŠ” ê²ƒì„ ê¸°ëŒ€í•˜ê¸° ë•Œë¬¸ì— ì•„ëž˜ì™€ ê°™ì´ ë³€í™˜ìž‘ì—…ì„ í•´ì£¼ì–´ì•¼ í•œë‹¤.

```python
import numpy

train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T
```

ìœ„ ê³¼ì •ì„ ê±°ì¹˜ê³  ë‚˜ê²Œ ë˜ë©´, í•˜ë‚˜ì˜ ì—´ì„ ê°€ì§€ëŠ” ë²¡í„°ê°€ ìƒì„±ì´ ëœë‹¤. ì´ë¯¸ì§€ì˜ ê²½ìš° 0~255 ì‚¬ì´ì˜ ê°’ì„ ê°–ìœ¼ë¯€ë¡œ, ì´ ê°’ì„ í‘œì¤€í™”(ë˜ëŠ” ì •ê·œí™”) ì‹œí‚¤ê¸° ìœ„í•´ 255 ë¡œ ë‚˜ëˆ ì£¼ì—ˆë‹¤.

#### ì™œ ì •ê·œí™”ë‚˜ í‘œì¤€í™”ê°€ í•„ìš”í•œê°€?

ë°ì´í„°ì˜ ìŠ¤ì¼€ì¼ì´ ë„ˆë¬´ í¬ë©´, ê²½ì‚¬í•˜ê°•ë²•ì„ ì ìš©í•  ë•Œ ê¸°ìš¸ê¸°ê°€ ë„ˆë¬´ ë¹ ë¥´ê²Œ ì»¤ì§€ê±°ë‚˜ ë„ˆë¬´ë¹ ë¥´ê²Œ ìž‘ì•„ì ¸ ê°€ì¤‘ì¹˜ì˜ ì—…ë°ì´íŠ¸ ì†ë„ê°€ ëŠë ¤ì§„ë‹¤.
ë‹¤ìŒì˜ ì˜ˆì‹œì—ì„œ ì •ê·œí™”ê°€ ëœ ë°ì´í„°ì˜ ì†ì‹¤í•¨ìˆ˜ ë³€í™”ì™€ ì •ê·œí™”ê°€ ë˜ì§€ ì•Šì€ ë°ì´í„°ì˜ ì†ì‹¤í•¨ìˆ˜ ë³€í™”ë¥¼ ë³´ë©´ ì •ê·œí™”ëœ ë°ì´í„°ì˜ ê²½ìš° ì†ì‹¤í•¨ìˆ˜ê°€ ë§¤ë„ëŸ½ê²Œ ìž‘ì•„ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìžˆì§€ë§Œ, ì •ê·œí™”ê°€ ë˜ì§€ ì•Šì€ ì†ì‹¤í•¨ìˆ˜ëŠ” ì†ì‹¤í•¨ìˆ˜ í¬ê¸°ê°€ ë¹„ì´ìƒì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìžˆë‹¤.

![figure1](./assets/Figure_1.png)

{{% details title="Code Example" closed="true" %}}

```python
import numpy as np
import matplotlib.pyplot as plt


np.random.seed(42)
m = 100
X = np.random.rand(1, m) * 100
Y = (3 * X + 7 + np.random.randn(1, m) * 10)


W1, b1 = 0, 0
alpha = 0.0001
costs1 = []

for i in range(1000):
    Z1 = np.dot(W1, X) + b1
    dW1 = (1/m) * np.dot((Z1 - Y), X.T)
    db1 = (1/m) * np.sum(Z1 - Y)

    W1 -= alpha * dW1
    b1 -= alpha * db1

    cost = (1/(2*m)) * np.sum((Z1 - Y) ** 2)
    costs1.append(cost)


X_standardized = (X - np.mean(X)) / np.std(X)

W2, b2 = 0, 0
costs2 = []

for i in range(1000):
    Z2 = np.dot(W2, X_standardized) + b2
    dW2 = (1/m) * np.dot((Z2 - Y), X_standardized.T)
    db2 = (1/m) * np.sum(Z2 - Y)

    W2 -= alpha * dW2
    b2 -= alpha * db2

    cost = (1/(2*m)) * np.sum((Z2 - Y) ** 2)
    costs2.append(cost)

plt.plot(costs1, label="Without Standardization", linestyle='dashed', color='red')
plt.plot(costs2, label="With Standardization", linestyle='solid', color='blue')
plt.xlabel("Iterations")
plt.ylabel("Cost")
plt.legend()
plt.title("Gradient Descent with & without Standardization")
plt.show()
```

{{% /details %}}

## Logistic Regressionì„ ì´ìš©í•´ ì´ë¯¸ì§€ê°€ ê³ ì–‘ì´ì¸ì§€ ì•„ë‹Œì§€ í•™ìŠµí•˜ê¸°

Logistic Regressionì€ **ì´ì§„ ë¶„ë¥˜(Binary Classification)** ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ,  
ìž…ë ¥ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•´ë‹¹ ë°ì´í„°ê°€ íŠ¹ì • í´ëž˜ìŠ¤(ì˜ˆ: ê³ ì–‘ì´ì¼ í™•ë¥ )ë¥¼ ê°€ì§ˆ í™•ë¥ ì„ ì¶œë ¥í•œë‹¤.

ì´ ëª¨ë¸ì€ **ìž…ë ¥ ë²¡í„°** \( x \) ì— ëŒ€í•´ **ê°€ì¤‘ì¹˜ \( w \) ì™€ ë°”ì´ì–´ìŠ¤ \( b \)** ë¥¼ ì ìš©í•œ  
**ì„ í˜• ë³€í™˜(linear transformation)** ì„ ìˆ˜í–‰í•œ í›„,

ì´í›„, **ì‹œê·¸ëª¨ì´ë“œ(Sigmoid) í•¨ìˆ˜**ë¥¼ ì ìš©í•˜ì—¬ **ì¶œë ¥ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥  ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤.**

$$z^{(i)} = w^T x^{(i)} + b$$
$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$

### ì†ì‹¤ í•¨ìˆ˜ (Loss Function)

**ì†ì‹¤ í•¨ìˆ˜** ëŠ” ì•„ëž˜ì™€ ê°™ì´ ì •ì˜ë˜ë©°,  
**ì˜ˆì¸¡ê°’ \( a^{(i)} \) ê°€ ì‹¤ì œê°’ \( y^{(i)} \) ì™€ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ë‹¤.**

$$ \mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \log(a^{(i)}) - (1-y^{(i)} ) \log(1-a^{(i)})$$

### ë¹„ìš© í•¨ìˆ˜ (Cost Function)

ì—¬ëŸ¬ ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤ ê°’ì„ í‰ê·  ë‚´ì–´ **ë¹„ìš© í•¨ìˆ˜** ë¥¼ ì •ì˜í•  ìˆ˜ ìžˆë‹¤.

$$ J = \frac{1}{m} \sum\_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$

## ðŸŸ¢ Forward Propagation

### Forward Propagationì„ ìˆ˜í–‰í•˜ëŠ” ì´ìœ 

- ìž…ë ¥ ë°ì´í„° \( X \) ë¥¼ ë°›ì•„ ì˜ˆì¸¡ê°’ \( A \) ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ìˆ˜í–‰í•œë‹¤.
- ì„ í˜• ë³€í™˜ê³¼ í™œì„±í™” í•¨ìˆ˜ë¥¼ ê±°ì³ ìµœì¢…ì ìœ¼ë¡œ í™•ë¥  ê°’(ì˜ˆì¸¡ê°’) \( A \) ë¥¼ ì–»ëŠ”ë‹¤.
- ì´í›„, ì´ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ \( Y \) ë¥¼ ë¹„êµí•˜ì—¬ ë¹„ìš© í•¨ìˆ˜ \( J \) ë¥¼ ê³„ì‚°í•œë‹¤.

### Forward Propagation ê³„ì‚° ê³¼ì •

1. **ì„ í˜• ì¡°í•© ê³„ì‚° (Linear Transformation)**
   $$
   z = w^T X + b
   $$
2. **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì ìš© (Sigmoid Activation)**
   $$
   A = \sigma(z) = \frac{1}{1 + e^{-z}}
   $$
3. **ë¹„ìš© í•¨ìˆ˜(Cost Function) ê³„ì‚°**
   $$
   J = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log a^{(i)} + (1 - y^{(i)}) \log(1 - a^{(i)}) \right]
   $$

---

## ðŸ”´ Backward Propagation

### Backward Propagationì„ ìˆ˜í–‰í•˜ëŠ” ì´ìœ 

- ë¹„ìš© í•¨ìˆ˜ \( J \) ë¥¼ \( w \) ì™€ \( b \) ì— ëŒ€í•´ ë¯¸ë¶„í•˜ì—¬ Gradientë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ìˆ˜í–‰í•œë‹¤.
- ì´ Gradientë¥¼ ì´ìš©í•´ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ìœ¼ë¡œ ê°€ì¤‘ì¹˜ \( w \) ì™€ ë°”ì´ì–´ìŠ¤ \( b \) ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
- ì¦‰, **ëª¨ë¸ì´ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •!**

### Backward Propagation ê³„ì‚° ê³¼ì •

1. **Cost Functionì„ ì˜ˆì¸¡ê°’ \( A \) ì— ëŒ€í•´ ë¯¸ë¶„**

   $$
   \frac{\partial J}{\partial A} = \frac{1}{m} (A - Y)
   $$

2. **Sigmoid í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„**

   $$
   \frac{\partial A}{\partial z} = A(1 - A)
   $$

3. **ì„ í˜• ë³€í™˜ì˜ ë¯¸ë¶„**

   $$
   \frac{\partial z}{\partial w} = X
   $$

4. **ìµœì¢… Gradient ê³„ì‚° (íŽ¸ë¯¸ë¶„ ì ìš©)**

   $$
   \frac{\partial J}{\partial w} = \frac{1}{m} X (A - Y)^T
   $$

5. **ë°”ì´ì–´ìŠ¤ \( b \) ì— ëŒ€í•œ ë¯¸ë¶„**
   $$
   \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} (A - Y)
   $$

### \( (A - Y)^T \)ì—ì„œ Transpose (\( T \))ê°€ í•„ìš”í•œ ì´ìœ 

ìš°ë¦¬ê°€ ì‚¬ìš©í•´ì•¼ í•  ë¯¸ë¶„ ê³µì‹ ì¤‘ í•˜ë‚˜ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤:

$$
\frac{\partial J}{\partial w} = \frac{1}{m} X (A - Y)^T
$$

ì—¬ê¸°ì„œ **ì™œ Transposeê°€ í•„ìš”í•œì§€** ì‚´íŽ´ë³´ë©´,

### ** í–‰ë ¬ í¬ê¸° ë¶„ì„**

| ë³€ìˆ˜        | í¬ê¸° (Shape)               |
| ----------- | -------------------------- |
| \( X \)     | \( (\{num_features}, m) \) |
| \( A \)     | \( (1, m) \)               |
| \( Y \)     | \( (1, m) \)               |
| \( A - Y \) | \( (1, m) \)               |

#### ** ë§Œì•½ Transpose ì—†ì´ ì—°ì‚°ì„ í•˜ë©´?**

ë§Œì•½ Transpose ì—†ì´ ê³±í•˜ë©´:

$$
X (A - Y) = ({num_features}, m) * (1, m)
$$

**ì´ ì—°ì‚°ì€ ì„±ë¦½í•˜ì§€ ì•ŠìŒ!**  
í–‰ë ¬ ê³±ì…ˆì—ì„œ **ë‚´ë¶€ ì°¨ì› (m) ì´ ë§žì§€ ì•Šê¸° ë•Œë¬¸**ì´ë‹¤.

### \( A(1 - A) \) ê°€ ì‚¬ë¼ì§€ëŠ” ì´ìœ 

ìš°ë¦¬ê°€ Chain Ruleì„ ì ìš©í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì€ ì—°ì‚°ì„ í•œë‹¤.

$$
\frac{\partial J}{\partial w} = \frac{\partial J}{\partial A} \cdot \frac{\partial A}{\partial z} \cdot \frac{\partial z}{\partial w}
$$

ì—¬ê¸°ì„œ **\( \frac{\partial A}{\partial z} \) ë¥¼ ê³„ì‚°í•˜ë©´?**

$$
\frac{\partial A}{\partial z} = A(1 - A)
$$

ê·¸ëŸ°ë°, ìš°ë¦¬ê°€ êµ¬í•œ \( \frac{\partial J}{\partial A} \) ëŠ”:

$$
\frac{\partial J}{\partial A} = \frac{1}{m} (A - Y)
$$

ì´ì œ **ë‘˜ì„ ê³±í•˜ë©´?**

$$
\frac{1}{m} (A - Y) \cdot A(1 - A)
$$

ì´ ì‹ì—ëŠ” ë¶„ëª… **\( A(1 - A) \) í•­ì´ í¬í•¨ë˜ì–´ ìžˆìŒ!**  
í•˜ì§€ë§Œ, ìµœì¢… ë¹„ìš© í•¨ìˆ˜ì˜ ë¯¸ë¶„ì„ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´ **ê²°êµ­ \( A(1 - A) \) í•­ì´ ì—†ì–´ì§€ëŠ” í˜•íƒœë¡œ ë³€í˜•ëœë‹¤.**

### **ì™œ ì—†ì–´ì§ˆê¹Œ?**

- \( A(1 - A) \) í•­ì´ ìžˆë”ë¼ë„, **Gradient ë°©í–¥ì€ \( (A - Y) \) ë¡œ ê²°ì •ë¨**.
- ì¦‰, **Gradientì˜ ë¶€í˜¸(sign)ëŠ” ë°”ë€Œì§€ ì•ŠìŒ** â†’ í•™ìŠµ ë°©í–¥ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ.
- ë˜í•œ, í•™ìŠµ ì†ë„ ì¡°ì ˆì€ **Learning Rate (\(\alpha\)) ë¡œ ì¶©ë¶„ížˆ ê°€ëŠ¥**í•˜ë¯€ë¡œ **ë¶ˆí•„ìš”í•œ í•­ì„ ì œê±°í•˜ê³  ìµœì í™”í•  ìˆ˜ ìžˆìŒ.**
- ê·¸ëž˜ì„œ ì‹¤ì œ ì½”ë“œì—ì„œëŠ” **\( A(1 - A) \) í•­ ì—†ì´ \( (A - Y) \) ë§Œ ë‚¨ê¸°ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¨ìˆœí™”ë¨.**

**ì¦‰, í•™ìŠµí•  ë•Œ ê¼­ í•„ìš”í•œ ìš”ì†Œê°€ ì•„ë‹ˆë¯€ë¡œ ìƒëžµí•˜ëŠ” ê²ƒì´ê³ , í•™ìŠµ ê²°ê³¼ì—ë„ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ!**

---

## Gradient Descent (ê²½ì‚¬ í•˜ê°•ë²•)

1. **ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**

   $$
   w = w - \alpha \frac{\partial J}{\partial w}
   $$

2. **ë°”ì´ì–´ìŠ¤ ì—…ë°ì´íŠ¸**
   $$
   b = b - \alpha \frac{\partial J}{\partial b}
   $$

- \( \alpha \) : í•™ìŠµë¥  (Learning Rate), ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ì—…ë°ì´íŠ¸í• ì§€ ê²°ì •

## ì½”ë“œ

```python
import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def initialize_with_zeros(dim):
    w = np.zeros((dim,1))
    b = 0
    return w, b

def propagate(w,b,X,Y):
    """Computes cost and gradient"""
    m = X.shape[1] # number of samples

    A = sigmoid(np.dot(w.T, X)+ b)
    cost = - (1/m) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))

    dw = (1/m) * np.dot(X, (A-Y).T)
    db = (1/m) * np.sum(A-Y)

    grads = {"dw": dw, "db": db}
    return grads, cost

def optimize(w,b,X,Y,num_iterations=1000, learning_rate, print_cost=False):
    """Performs gradient descent to update parameters"""
    costs = []
    for i in range(num_iterations):
        grads, cost = propagate(w,b,X,Y)

        dw = grads['dw']
        db = grads['db']

        w = w - dw * learning_rate
        b = b - dw * learning_rate

        costs.append(cost)


    params = {"w": w, "b": b}
    grads = {"dw": dw, "db": db}

    return params, grads, costs

def predict(w, b, X):
    """Predicts labels using learned logistic regression parameters"""
    m = X.shape[1]
    Y_prediction = np.zeros((1, m))

    A = sigmoid(np.dot(w.T, X) + b)

    Y_prediction = (A > 0.5).astype(int)

    return Y_prediction

def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):
    """Builds and trains the logistic regression model"""
    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)

    w = params['w']
    b = params['b']

    Y_prediction_train = predict(w, b, X_train)
    Y_prediction_test = predict(w, b, X_test)

    d = {
        "costs": costs,
        "Y_prediction_test": Y_prediction_test,
        "Y_prediction_train": Y_prediction_train,
        "w": w,
        "b": b,
        "learning_rate": learning_rate,
        "num_iterations": num_iterations
    }

    return d
```
